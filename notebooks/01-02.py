# -*- coding: utf-8 -*-
"""01 - 02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bLL4O1Tp6y_F4v3g0fjEv9xUr2XRKDuk

# TAHAP 1 - Membangun Case Base

# Scrapping Data
"""

from IPython import get_ipython
from IPython.display import display

!pip install pandas requests beautifulsoup4 pdfminer.six lxml > /dev/null 2>&1

from google.colab import drive
drive.mount('/content/drive')

import argparse
import io
import os
import re
import time
import urllib
from concurrent.futures import ThreadPoolExecutor, wait
from datetime import date
import pandas as pd
import requests
from bs4 import BeautifulSoup
from pdfminer import high_level

def create_path(folder_name):
    path = os.path.join(os.getcwd(), folder_name)
    if not os.path.exists(path):
        os.makedirs(path)
    return path

def open_page(link):
    count = 0
    while count < 3:
        try:
            return BeautifulSoup(requests.get(link).text, "lxml")
        except:
            count += 1
            time.sleep(5)

def get_detail(soup, keyword):
    try:
        text = (
            soup.find(lambda tag: tag.name == "td" and keyword in tag.text)
            .find_next()
            .get_text()
            .strip()
        )
        return text
    except:
        return ""

def get_pdf(url, path_pdf):
    file = urllib.request.urlopen(url)
    file_name = file.info().get_filename().replace("/", " ")
    file_content = file.read()
    with open(f"{path_pdf}/{file_name}", "wb") as out_file:
        out_file.write(file_content)
    return io.BytesIO(file_content), file_name

def clean_text(text):
    text = text.replace("M a h ka m a h A g u n g R e p u blik In d o n esia\n", "")
    text = text.replace("Disclaimer\n", "")
    text = text.replace(
        "Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\n",
        "",
    )
    text = text.replace(
        "pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\n",
        "",
    )
    text = text.replace(
        "Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\n",
        "",
    )
    text = text.replace(
        "Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\n",
        "",
    )
    return text

def is_url_already_scraped(url, destination):
    """
    Checks if a URL has already been scraped and saved in the CSV file.

    Args:
        url (str): The URL to check.
        destination (str): The path to the output CSV file.

    Returns:
        bool: True if the URL exists in the CSV, False otherwise.
    """
    if not os.path.isfile(f"{destination}.csv"):
      return False

    try:
        df = pd.read_csv(f"{destination}.csv")
        return url in df["link"].values
    except pd.errors.EmptyDataError:
      return False

def extract_data(link, keyword_url):
    global today
    global path_output
    global path_pdf
    global download_pdf

    path_output = '/content/drive/MyDrive/DokumenPutusanMA/CSV'
    path_pdf = '/content/drive/MyDrive/DokumenPutusanMA/PDF'
    today = date.today().strftime("%Y-%m-%d")

    keyword_url = keyword_url.replace("/", " ")
    if keyword_url.startswith("https"):
        keyword_url = ""
    destination = f"{path_output}/putusan_ma_{keyword_url}_{today}"

    if is_url_already_scraped(link, destination):
        print(f"Skipping duplicate URL: {link}")
        return

    soup = open_page(link)
    table = soup.find("table", {"class": "table"})
    judul = table.find("h2").text
    table.find("h2").decompose()

    nomor = get_detail(table, "Nomor")
    tingkat_proses = get_detail(table, "Tingkat Proses")
    klasifikasi = get_detail(table, "Klasifikasi")
    kata_kunci = get_detail(table, "Kata Kunci")
    tahun = get_detail(table, "Tahun")
    tanggal_register = get_detail(table, "Tanggal Register")
    lembaga_peradilan = get_detail(table, "Lembaga Peradilan")
    jenis_lembaga_peradilan = get_detail(table, "Jenis Lembaga Peradilan")
    hakim_ketua = get_detail(table, "Hakim Ketua")
    hakim_anggota = get_detail(table, "Hakim Anggota")
    panitera = get_detail(table, "Panitera")
    amar = get_detail(table, "Amar")
    amar_lainnya = get_detail(table, "Amar Lainnya")
    catatan_amar = get_detail(table, "Catatan Amar")
    tanggal_musyawarah = get_detail(table, "Tanggal Musyawarah")
    tanggal_dibacakan = get_detail(table, "Tanggal Dibacakan")
    kaidah = get_detail(table, "Kaidah")
    abstrak = get_detail(table, "Abstrak")

    try:
        link_pdf = soup.find("a", href=re.compile(r"/pdf/"))["href"]
        file_pdf, file_name_pdf = get_pdf(link_pdf, path_pdf)
        text_pdf = high_level.extract_text(file_pdf)
        text_pdf = clean_text(text_pdf)
    except:
        link_pdf = ""
        text_pdf = ""
        file_name_pdf = ""

    data = [
        judul,
        nomor,
        tingkat_proses,
        klasifikasi,
        kata_kunci,
        tahun,
        tanggal_register,
        lembaga_peradilan,
        jenis_lembaga_peradilan,
        hakim_ketua,
        hakim_anggota,
        panitera,
        amar,
        amar_lainnya,
        catatan_amar,
        tanggal_musyawarah,
        tanggal_dibacakan,
        kaidah,
        abstrak,
        link,
        link_pdf,
        file_name_pdf,
        text_pdf,
    ]
    result = pd.DataFrame(
        [data],
        columns=[
            "judul",
            "nomor",
            "tingkat_proses",
            "klasifikasi",
            "kata_kunci",
            "tahun",
            "tanggal_register",
            "lembaga_peradilan",
            "jenis_lembaga_peradilan",
            "hakim_ketua",
            "hakim_anggota",
            "panitera",
            "amar",
            "amar_lainnya",
            "catatan_amar",
            "tanggal_musyawarah",
            "tanggal_dibacakan",
            "kaidah",
            "abstrak",
            "link",
            "link_pdf",
            "file_name_pdf",
            "text_pdf",
        ],
    )

    print(destination)
    if not os.path.isfile(f"{destination}.csv"):
        result.to_csv(f"{destination}.csv", header=True, index=False)
    else:
        result.to_csv(f"{destination}.csv", mode="a", header=False, index=False)

def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):
    if not keyword and not url:
        print("Please provide a keyword or URL")
        return

    path_output = '/content/drive/MyDrive/DokumenPutusanMA/CSV'
    path_pdf = '/content/drive/MyDrive/DokumenPutusanMA/PDF'

    today = date.today().strftime("%Y-%m-%d")

    link = f"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1"

    if url:
        link = url

    soup = open_page(link)

    page_links = soup.find_all("a", {"class": "page-link"})
    if not page_links:
        last_page = 1
    else:
        last_page = int(page_links[-1].get("data-ci-pagination-page"))

    if url:
        print(f"Scraping with url: {url} - {20 * last_page} data - {last_page} page")
    else:
        print(f"Scraping with keyword: {keyword} - {20 * last_page} data - {last_page} page")

    if url:
        keyword_url = url
    else:
        keyword_url = keyword

    futures = []
    with ThreadPoolExecutor(max_workers=4) as executor:
        for page_num in range(1, last_page + 1):
            futures.append(
                executor.submit(run_process, keyword_url, page_num, sort_date)
            )
    wait(futures)

def run_process(keyword_url, page, sort_page):
    if keyword_url.startswith("https"):
        link = f"{keyword_url}&page={page}"
    else:
        link = f"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}"
    if sort_page:
        link = f"{link}&obf=TANGGAL_PUTUS&obm=desc"

    print(link)

    soup = open_page(link)
    links = soup.find_all("a", {"href": re.compile("/direktori/putusan")})

    for link in links:
        extract_data(link["href"], keyword_url)

def scrape_specific_url(url, download_pdf=True):
    if not url or not url.startswith("https://"):
        print("Please provide a valid URL")
        return

    path_output = '/content/drive/MyDrive/DokumenPutusanMA/CSV'
    path_pdf = '/content/drive/MyDrive/DokumenPutusanMA/PDF'
    today = date.today().strftime("%Y-%m-%d")

    extract_data(url, url, path_output, path_pdf, today)

run_scraper(url="https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=098167PN337+++++++++++++++++++++&t_put=2025&t_reg=&t_upl=&t_pr=https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=3c40e48bbab311301a21c445b3c7fe57&jd=&tp=0&court=098167PN337+++++++++++++++++++++&t_put=2025&t_reg=&t_upl=&t_pr=")

"""# Konversi dan Ekstraksi Teks"""

!pip install pdfminer.six

from pdfminer.high_level import extract_text
import os

pdf_folder = '/content/drive/MyDrive/DokumenPutusanMA/PDF'
output_folder = '/content/drive/MyDrive/DokumenPutusanMA/RawPDF'

os.makedirs(output_folder, exist_ok=True)

for filename in os.listdir(pdf_folder):
    if filename.endswith('.pdf'):
        case_id = filename.replace('.pdf', '')
        input_path = os.path.join(pdf_folder, filename)
        output_path = os.path.join(output_folder, f"{case_id}.txt")

        try:
            text = extract_text(input_path)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(text)
        except Exception as e:
            print(f"Error converting {filename}: {e}")

!pip install regex

import re
import os

raw_folder = '/content/drive/MyDrive/DokumenPutusanMA/RawPDF'
cleaned_folder = '/content/drive/MyDrive/DokumenPutusanMA/CleanedRawPDF'
log_path = '/content/drive/MyDrive/DokumenPutusanMA/logs/cleaning.log'

os.makedirs(cleaned_folder, exist_ok=True)

log_dir = os.path.dirname(log_path)
os.makedirs(log_dir, exist_ok=True)

# Check if log_path exists and is a directory, and remove it if so
if os.path.exists(log_path) and os.path.isdir(log_path):
    print(f"Removing existing directory at log path: {log_path}")
    os.rmdir(log_path) # Use os.rmdir() for empty directories, os.removes for non-empty

log_file = open(log_path, 'w', encoding='utf-8')

def clean_text(text):
    text = re.sub(r'\n+', '\n', text)                    # hapus baris kosong berlebihan
    text = re.sub(r'\s+', ' ', text)                     # normalisasi spasi
    text = re.sub(r'\d+\s*/\s*\d+', '', text)            # hapus nomor halaman
    text = re.sub(r'halaman \d+', '', text, flags=re.I)  # hapus watermark 'Halaman x'
    text = re.sub(r'[^\w\s.,]', '', text)                # hapus simbol aneh (optional)
    text = text.lower()                                  # lowercase
    return text.strip()

for file in os.listdir(raw_folder):
    if file.endswith('.txt'):
        file_path = os.path.join(raw_folder, file)
        with open(file_path, 'r', encoding='utf-8') as f:
            original_text = f.read()

        cleaned_text = clean_text(original_text)

        with open(os.path.join(cleaned_folder, file), 'w', encoding='utf-8') as f:
            f.write(cleaned_text)

        log_file.write(f"{file}: cleaned, original length={len(original_text)}, cleaned length={len(cleaned_text)}\n")

log_file.close()

"""# TAHAP 2 - Case Representation"""

import os
import re
import pandas as pd

RAW_DIR = '/content/drive/MyDrive/DokumenPutusanMA/CleanedRawPDF' # Sesuaikan dengan folder Anda
OUTPUT_PATH = '/content/drive/MyDrive/DokumenPutusanMA/processed/Kasus_narkotika3.csv'

# Fungsi bantu ekstraksi dari teks
def extract_metadata(text):
    tanggal = re.search(r'tanggal[\s:]*([0-9]{1,2} [a-z]+ 20[0-9]{2})', text.lower())
    pihak = re.search(r'nama lengkap\s*:?\s*(.*?)\s*(tempat lahir|umur|jenis kelamin)', text.lower(), re.DOTALL)

    return {
        'tanggal': tanggal.group(1) if tanggal else '',
        'pihak': pihak.group(1).strip() if pihak else '',
    }

def extract_no_perkara(text):
    # Coba cari format normal dulu
    match = re.search(r'(putusan\s*)?(nomor|no)[\s:]*([0-9]+[^\s\n]*)', text.lower())
    if match:
        return match.group(3).strip()

    # Fallback: cari pola seperti "nomor 53pid.sus2025pn skt" langsung
    fallback = re.search(r'nomor\s+([0-9]{1,4}pid[.\s\-]?sus[0-9]{4}pn\s?[a-z]+)', text.lower())
    if fallback:
        return fallback.group(1).replace(' ', '').replace('.', '').replace('-', '')

    return ''


# Ringkasan Fakta
def extract_summary(text):
    keyword_idx = text.lower().find("bermula pada")
    if keyword_idx == -1:
        keyword_idx = text.lower().find("bahwa pada")
    if keyword_idx == -1:
        keyword_idx = text.lower().find("menimbang")
    if keyword_idx == -1:
        return ''
    return text[keyword_idx: keyword_idx+500] + "..."

def extract_pasals(text):
    text = text.lower()
    patterns = [
        r'pasal\s+\d+\s*(ayat\s*\(?\d+\)?)?\s*(jo\s+pasal\s+\d+\s*(ayat\s*\(?\d+\)?)?)?\s*uu\s*[^.\n]{5,60}',
        r'melanggar\s+pasal\s+\d+(?:\s*ayat\s*\(?\d+\)?)?',
        r'pasal\s+\d+(?:\s*ayat\s*\(?\d+\)?)?(?:\s*jo\s*pasal\s+\d+(?:\s*ayat\s*\(?\d+\)?)*?)?',
    ]

    results = []
    for pattern in patterns:
        # Re.findall with groups returns a list of tuples. Join tuple elements into a string
        found_matches = re.findall(pattern, text)
        # For patterns with groups, we need to join the elements of each tuple
        if found_matches and isinstance(found_matches[0], tuple):
            results.extend([''.join(tup) for tup in found_matches])
        else:
            # For patterns without groups, re.findall returns a list of strings
            results.extend(found_matches)


    # Gabungkan hasil (hapus duplikat), rapikan spasi
    # Ensure all elements in results are strings before stripping
    cleaned_results = [str(p).strip() for p in results if str(p).strip()]
    return '; '.join(set(cleaned_results))


def extract_amar_putusan(text):
    last_part = text[-10000:].lower()

    # Pola 1: cocokkan paragraf amar yang eksplisit dengan kata penting seperti penjara, denda, dll.
    pattern_1 = r"(menyatakan.*?(penjara|dend[a|e]|rupiah|dirampas|dimusnahkan|biaya perkara).*?)(?=(menimbang|dasar hukum|$))"
    match_1 = re.search(pattern_1, last_part, re.DOTALL)

    if match_1:
        amar = match_1.group(1)
        amar = re.sub(r'\s+', ' ', amar).strip()
        return amar

    # Pola 2: fallback jika tidak ditemukan dari pola pertama
    pattern_2 = r"(1\.\s*menyatakan.*?biaya perkara[^.]+(?:\.)?)"
    match_2 = re.search(pattern_2, last_part, re.DOTALL)

    if match_2:
        amar = match_2.group(1)
        amar = re.sub(r'\s+', ' ', amar).strip()
        return amar

    return ''


# Proses semua file di folder raw
data = []
for fname in os.listdir(RAW_DIR):
    if not fname.endswith('.txt'):
        continue
    case_id = fname.split('.')[0].replace('case_', '')
    with open(os.path.join(RAW_DIR, fname), 'r', encoding='utf-8') as f:
        text = f.read()

    meta = extract_metadata(text)
    summary = extract_summary(text)
    # Call extract_no_perkara here and assign the result to no_perkara
    no_perkara = extract_no_perkara(text)
    # Call extract_pasals here to get the pasal information
    pasals = extract_pasals(text)
    amar = extract_amar_putusan(text)
    text_len = len(text.split())

    if ('narkotika' not in text.lower() and 'psikotropika' not in text.lower()) or not meta['pihak']:
        continue

    data.append({
        'case_id': case_id,
        'no_perkara': no_perkara,  # Now no_perkara is defined
        'tanggal': meta['tanggal'],
        'ringkasan_fakta': summary,
        'pasal': pasals, # Use the variable storing the return value
        'pihak': meta['pihak'],
        'amar_putusan': amar,
        'length_words': text_len,
        'text_full': text[:1000] + '...'
    })

# Simpan ke CSV
df = pd.DataFrame(data)
os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8-sig')

print(f"✅ Berhasil disimpan ke {OUTPUT_PATH} dengan {len(df)} kasus valid.")