# -*- coding: utf-8 -*-
"""03_retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W1RfZpRkvzdU9rwJp5N3V4F_MJRczTZ-
"""

import pandas as pd
import numpy as np
import re
from transformers import BertTokenizer, TFBertModel
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from scipy.spatial.distance import cdist
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os

df = pd.read_csv('/content/Kasus_narkotika3.csv')

# Definisikan mapping teks ke angka
teks_to_angka = {
    'satu': 1, 'dua': 2, 'tiga': 3, 'empat': 4, 'lima': 5,
    'enam': 6, 'tujuh': 7, 'delapan': 8, 'sembilan': 9, 'sepuluh': 10,
    'sebelas': 11, 'dua belas': 12, 'tiga belas': 13, 'empat belas': 14
}

def convert_teks_to_angka(text):
    """Konversi teks angka ke numerik (misal: 'lima' -> 5)"""
    if not isinstance(text, str):
        return None
    text = text.lower().strip()
    return teks_to_angka.get(text, None)

def extract_tahun(text):
    """Ekstrak jumlah tahun dari teks, mendukung numerik dan teks"""
    if not isinstance(text, str):
        return None
    text = text.lower()
    total_tahun = 0.0

    # Ekstrak tahun numerik
    match_numerik = re.search(r'(\d+\.?\d*)\s*(tahun|thn)', text)
    if match_numerik:
        try:
            total_tahun += float(match_numerik.group(1))
        except ValueError:
            pass

    # Ekstrak tahun dalam teks (jika belum ada numerik)
    match_teks = re.search(r'\b(satu|dua|tiga|empat|lima|enam|tujuh|delapan|sembilan|sepuluh|sebelas|dua belas|tiga belas|empat belas)\b\s*(tahun|thn)', text)
    if match_teks and total_tahun == 0:
        angka = convert_teks_to_angka(match_teks.group(1))
        if angka:
            total_tahun += angka

    # Ekstrak bulan numerik
    match_bulan_numerik = re.search(r'(\d+)\s*bulan', text)
    if match_bulan_numerik:
        try:
            bulan = float(match_bulan_numerik.group(1))
            total_tahun += bulan / 12
        except ValueError:
            pass

    # Ekstrak bulan dalam teks
    match_bulan_teks = re.search(r'\b(satu|dua|tiga|empat|lima|enam|tujuh|delapan|sembilan)\b\s*bulan', text)
    if match_bulan_teks:
        bulan = convert_teks_to_angka(match_bulan_teks.group(1))
        if bulan:
            total_tahun += bulan / 12

    return total_tahun if total_tahun > 0 else None

def label_kategori(text):
    """Menentukan kategori hukuman berdasarkan teks putusan"""
    if text is None or pd.isna(text):
        return 'Lainnya'

    text = str(text).lower()

    if 'mati' in text or 'seumur' in text:
        return 'Hukuman Berat'

    tahun = extract_tahun(text)
    if tahun is not None:
        if tahun >= 10:
            return 'Hukuman Berat'
        elif 5 <= tahun < 10:
            return 'Hukuman Sedang'
        else:
            return 'Hukuman Ringan'

    if 'rehabilitasi' in text or 'percobaan' in text:
        return 'Hukuman Ringan'

    return 'Lainnya'

# Uji fungsi label_kategori
text = "5 tahun 6 bulan"
print(f"Input: {text} -> Kategori: {label_kategori(text)}")

# Tambahkan kolom label_kategori ke DataFrame
df['label_kategori'] = df['amar_putusan'].apply(label_kategori)
df = df[df['label_kategori'] != 'Lainnya'].reset_index(drop=True)

# Cetak distribusi label kategori
print("Distribusi label kategori:")
print(df['label_kategori'].value_counts())

# Persiapkan data untuk model
texts = df['amar_putusan'].astype(str).tolist()
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['label_kategori'])
labels = df['label'].tolist()
case_solutions = dict(zip(df['case_id'], df['amar_putusan']))
case_categories = dict(zip(df['case_id'], df['label_kategori']))

# Inisialisasi tokenizer dan model BERT
try:
    tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')
    bert_embedding_model = TFBertModel.from_pretrained('indobenchmark/indobert-base-p2')
    for layer in bert_embedding_model.layers:
        layer.trainable = False
except Exception as e:
    print(f"Error loading BERT model or tokenizer: {e}")
    exit(1)

# Definisikan BertLayer
class BertLayer(tf.keras.layers.Layer):
    def __init__(self, bert_model, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.bert = bert_model

    def call(self, inputs):
        input_ids, attention_mask = inputs
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, training=False)
        return output.last_hidden_state[:, 0, :]  # CLS token embedding

# Fungsi untuk mendapatkan embedding BERT
def get_bert_embeddings(text_list, tokenizer, bert_model):
    """Menghasilkan embedding BERT untuk daftar teks"""
    try:
        tokens = tokenizer(
            text_list,
            padding='max_length',
            truncation=True,
            max_length=256,
            return_tensors='tf'
        )
        output = bert_model(input_ids=tokens["input_ids"],
                          attention_mask=tokens["attention_mask"],
                          training=False)
        return output.last_hidden_state[:, 0, :].numpy()
    except Exception as e:
        print(f"Error generating BERT embeddings: {e}")
        return None

# Generate embedding untuk case base
print("\n--- Representasi Vektor (BERT Embedding) ---")
db_embeddings = get_bert_embeddings(texts, tokenizer, bert_embedding_model)
if db_embeddings is None:
    print("Gagal menghasilkan BERT embeddings.")
    exit(1)
print("BERT embeddings for the case base generated.")

# Persiapkan data untuk klasifikasi
full_input_ids = tokenizer(texts, padding='max_length', truncation=True, max_length=256, return_tensors='np')['input_ids']
full_attention_masks = tokenizer(texts, padding='max_length', truncation=True, max_length=256, return_tensors='np')['attention_mask']
full_labels = np.array(labels)
full_texts = np.array(texts)

# Inisialisasi Stratified K-Fold
n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
print(f"\n--- Splitting Data (5-Fold Stratified Cross-Validation) ---")
print(f"Data will be split into {n_splits} folds for cross-validation.")

# BERT Classifier dengan 5-Fold Cross-Validation
print("\n--- Model Retrieval (BERT Classifier dengan 5-Fold Cross-Validation) ---")
bert_accuracies = []
bert_class_reports = []
bert_cms = []

for fold, (train_index, test_index) in enumerate(skf.split(full_input_ids, full_labels)):
    print(f"\n--- Fold {fold + 1}/{n_splits} untuk BERT Classifier ---")

    X_train_ids, X_test_ids = full_input_ids[train_index], full_input_ids[test_index]
    X_train_masks, X_test_masks = full_attention_masks[train_index], full_attention_masks[test_index]
    y_train, y_test = full_labels[train_index], full_labels[test_index]

    # Bangun model BERT
    bert_base_model = TFBertModel.from_pretrained('indobenchmark/indobert-base-p2')
    for layer in bert_base_model.layers:
        layer.trainable = False

    input_ids_layer = Input(shape=(256,), dtype=tf.int32, name="input_ids")
    attention_mask_layer = Input(shape=(256,), dtype=tf.int32, name="attention_mask")
    bert_layer_instance = BertLayer(bert_base_model)
    outputs = bert_layer_instance([input_ids_layer, attention_mask_layer])
    x = Dense(128, activation='relu')(outputs)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    output = Dense(len(label_encoder.classes_), activation='softmax')(x)

    model_bert_cv = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output)
    model_bert_cv.compile(optimizer=Adam(learning_rate=2e-5),
                         loss='sparse_categorical_crossentropy',
                         metrics=['accuracy'])

    # Latih model
    model_bert_cv.fit(
        x={"input_ids": X_train_ids, "attention_mask": X_train_masks},
        y=y_train,
        epochs=10,
        batch_size=16,
        verbose=0
    )

    # Prediksi dan evaluasi
    y_pred_probs = model_bert_cv.predict({"input_ids": X_test_ids, "attention_mask": X_test_masks}, verbose=0)
    y_pred_bert = np.argmax(y_pred_probs, axis=1)

    fold_accuracy = accuracy_score(y_test, y_pred_bert)
    bert_accuracies.append(fold_accuracy)
    fold_report = classification_report(y_test, y_pred_bert, target_names=label_encoder.classes_, output_dict=True, zero_division=0)
    bert_class_reports.append(fold_report)
    bert_cms.append(confusion_matrix(y_test, y_pred_bert))

    print(f"Accuracy Fold {fold + 1}: {fold_accuracy * 100:.2f}%")
    print(f"Classification Report Fold {fold + 1}:")
    print(classification_report(y_test, y_pred_bert, target_names=label_encoder.classes_, zero_division=0))

# Rata-rata metrik BERT
print("\n--- Rata-rata Metrik BERT Classifier (5-Fold Cross-Validation) ---")
avg_bert_accuracy = np.mean(bert_accuracies)
print(f"Average Accuracy BERT Classifier: {avg_bert_accuracy * 100:.2f}%")

avg_bert_report = {cls: {'precision': [], 'recall': [], 'f1-score': []} for cls in label_encoder.classes_}
for report in bert_class_reports:
    for cls in label_encoder.classes_:
        if cls in report:
            avg_bert_report[cls]['precision'].append(report[cls]['precision'])
            avg_bert_report[cls]['recall'].append(report[cls]['recall'])
            avg_bert_report[cls]['f1-score'].append(report[cls]['f1-score'])

print("\nAverage Classification Report (per kelas):")
for cls in label_encoder.classes_:
    print(f"   {cls}:")
    print(f"     Precision: {np.mean(avg_bert_report[cls]['precision']) if avg_bert_report[cls]['precision'] else 0:.4f}")
    print(f"     Recall: {np.mean(avg_bert_report[cls]['recall']) if avg_bert_report[cls]['recall'] else 0:.4f}")
    print(f"     F1-Score: {np.mean(avg_bert_report[cls]['f1-score']) if avg_bert_report[cls]['f1-score'] else 0:.4f}")

# Visualisasi confusion matrix
sum_cm_bert = np.sum(bert_cms, axis=0)
plt.figure(figsize=(8, 6))
sns.heatmap(sum_cm_bert, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Summed Confusion Matrix BERT Classifier (5-Fold CV)')
plt.show()

# TF-IDF + SVM dengan 5-Fold Cross-Validation
print("\n--- Model Retrieval (TF-IDF + SVM dengan 5-Fold Cross-Validation) ---")
tfidf_svm_accuracies = []
tfidf_svm_class_reports = []
tfidf_svm_cms = []

for fold, (train_index, test_index) in enumerate(skf.split(full_texts, full_labels)):
    print(f"\n--- Fold {fold + 1}/{n_splits} untuk TF-IDF + SVM ---")

    X_train_text, X_test_text = full_texts[train_index], full_texts[test_index]
    y_train_tfidf, y_test_tfidf = full_labels[train_index], full_labels[test_index]

    tfidf_svm_model_cv = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('svm', SVC(kernel='linear', probability=True, random_state=42))
    ])

    tfidf_svm_model_cv.fit(X_train_text, y_train_tfidf)
    y_pred_tfidf_svm = tfidf_svm_model_cv.predict(X_test_text)

    fold_accuracy = accuracy_score(y_test_tfidf, y_pred_tfidf_svm)
    tfidf_svm_accuracies.append(fold_accuracy)
    fold_report = classification_report(y_test_tfidf, y_pred_tfidf_svm, target_names=label_encoder.classes_, output_dict=True, zero_division=0)
    tfidf_svm_class_reports.append(fold_report)
    tfidf_svm_cms.append(confusion_matrix(y_test_tfidf, y_pred_tfidf_svm))

    print(f"Accuracy Fold {fold + 1}: {fold_accuracy * 100:.2f}%")
    print(f"Classification Report Fold {fold + 1}:")
    print(classification_report(y_test_tfidf, y_pred_tfidf_svm, target_names=label_encoder.classes_, zero_division=0))

# Rata-rata metrik TF-IDF + SVM
print("\n--- Rata-rata Metrik TF-IDF + SVM Classifier (5-Fold Cross-Validation) ---")
avg_tfidf_svm_accuracy = np.mean(tfidf_svm_accuracies)
print(f"Average Accuracy TF-IDF + SVM Classifier: {avg_tfidf_svm_accuracy * 100:.2f}%")

avg_tfidf_svm_report = {cls: {'precision': [], 'recall': [], 'f1-score': []} for cls in label_encoder.classes_}
for report in tfidf_svm_class_reports:
    for cls in label_encoder.classes_:
        if cls in report:
            avg_tfidf_svm_report[cls]['precision'].append(report[cls]['precision'])
            avg_tfidf_svm_report[cls]['recall'].append(report[cls]['recall'])
            avg_tfidf_svm_report[cls]['f1-score'].append(report[cls]['f1-score'])

print("\nAverage Classification Report (per kelas):")
for cls in label_encoder.classes_:
    print(f"   {cls}:")
    print(f"     Precision: {np.mean(avg_tfidf_svm_report[cls]['precision']) if avg_tfidf_svm_report[cls]['precision'] else 0:.4f}")
    print(f"     Recall: {np.mean(avg_tfidf_svm_report[cls]['recall']) if avg_tfidf_svm_report[cls]['recall'] else 0:.4f}")
    print(f"     F1-Score: {np.mean(avg_tfidf_svm_report[cls]['f1-score']) if avg_tfidf_svm_report[cls]['f1-score'] else 0:.4f}")

# Visualisasi confusion matrix
sum_cm_tfidf_svm = np.sum(tfidf_svm_cms, axis=0)
plt.figure(figsize=(8, 6))
sns.heatmap(sum_cm_tfidf_svm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Summed Confusion Matrix TF-IDF + SVM (5-Fold CV)')
plt.show()

# Fungsi retrieval BERT
print("\n--- Fungsi Retrieval (BERT) ---")
def retrieve_bert(query: str, k: int = 5) -> tuple[list, list]:
    """Mengambil k kasus paling mirip menggunakan BERT embeddings"""
    query_embedding = get_bert_embeddings([query], tokenizer, bert_embedding_model)
    if query_embedding is None:
        print("Error generating query embedding.")
        return [], []

    query_embedding = query_embedding[0]
    sims = cosine_similarity([query_embedding], db_embeddings)[0]
    topk_idx = sims.argsort()[-k:][::-1]
    topk_case_ids = df.iloc[topk_idx]['case_id'].tolist()
    topk_similarities = sims[topk_idx].tolist()
    return topk_case_ids, topk_similarities

print("Fungsi `retrieve_bert()` siap digunakan.")

# Fungsi retrieval TF-IDF + SVM
print("\n--- Fungsi Retrieval (TF-IDF + SVM) ---")
def retrieve_tfidf_svm(query: str, k: int = 5) -> tuple[list, list]:
    """Mengambil k kasus paling mirip menggunakan TF-IDF + SVM"""
    global df, tfidf_svm_model_cv, label_encoder

    category_label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}

    # Validasi dependensi
    required_columns = ['case_id', 'amar_putusan', 'label_kategori', 'label']
    if not all(col in df.columns for col in required_columns):
        print(f"Error: df is missing required columns. Available columns: {df.columns}")
        return [], []
    if not hasattr(tfidf_svm_model_cv, 'named_steps'):
        print("Error: tfidf_svm_model_cv is not properly defined.")
        return [], []

    query = str(query).lower()
    tfidf_vectorizer = tfidf_svm_model_cv.named_steps['tfidf']
    query_tfidf = tfidf_vectorizer.transform([query])
    svm_model = tfidf_svm_model_cv.named_steps['svm']
    predicted_numerical_category = svm_model.predict(query_tfidf)[0]
    query_category = category_label_mapping.get(predicted_numerical_category, "Unknown Category")

    mask = df['label'] == predicted_numerical_category
    filtered_df = df[mask]
    filtered_indices = filtered_df.index.tolist()

    if len(filtered_indices) == 0:
        filtered_tfidf = tfidf_vectorizer.transform(df['amar_putusan'].astype(str).tolist())
        filtered_indices = df.index.tolist()

    filtered_tfidf = tfidf_vectorizer.transform(filtered_df['amar_putusan'].astype(str).tolist())
    sims = cosine_similarity(query_tfidf, filtered_tfidf)[0]
    topk_local_idx = np.argsort(sims)[-k:][::-1]
    topk_global_idx = [filtered_indices[i] for i in topk_local_idx]
    topk_case_ids = df.iloc[topk_global_idx]['case_id'].tolist()
    topk_similarities = sims[topk_local_idx].tolist()

    return topk_case_ids, topk_similarities

# Data uji
data_uji = [
    {
        "query_id": "Q1",
        "text": "Terdakwa membeli sabu untuk dijual kembali melalui kontak WhatsApp.",
        "ground_truth_case_ids": ["putusan_38_pid", "putusan_110_pid", "putusan_972_pid", "putusan_106_pid", "putusan_15_pid"]
    },
    {
        "query_id": "Q2",
        "text": "Terdakwa tertangkap dengan sabu seberat kurang dari 5 gram di dalam bungkus plastik klip.",
        "ground_truth_case_ids": ["putusan_110_pid", "putusan_118_pid", "putusan_61_pid", "putusan_27_pid", "putusan_54_pid"]
    },
    {
        "query_id": "Q3",
        "text": "Terdakwa memiliki sabu dengan berat lebih dari 5 gram untuk perantara jual beli.",
        "ground_truth_case_ids": ["putusan_116_pid", "putusan_3_pid", "putusan_882_pid", "putusan_972_pid", "putusan_754_pid", "putusan_62_pid", "putusan_67_pid"]
    },
    {
        "query_id": "Q4",
        "text": "Terdakwa menggunakan sabu untuk konsumsi pribadi dan diperintahkan rehabilitasi.",
        "ground_truth_case_ids": ["putusan_61_pid", "putusan_69_pid", "putusan_73_pid"]
    },
    {
        "query_id": "Q5",
        "text": "Terdakwa menyimpan sabu di dalam tas atau kardus saat ditangkap.",
        "ground_truth_case_ids": ["putusan_882_pid", "putusan_74_pid", "putusan_78_pid"]
    },
    {
        "query_id": "Q6",
        "text": "Terdakwa memiliki alat hisap sabu saat digerebek.",
        "ground_truth_case_ids": ["putusan_38_pid", "putusan_80_pid"]
    },
    {
        "query_id": "Q7",
        "text": "Terdakwa ditangkap dengan sabu di Kudus.",
        "ground_truth_case_ids": ["putusan_38_pid", "putusan_61_pid", "putusan_110_pid", "putusan_118_pid", "putusan_81_pid"]
    },
    {
        "query_id": "Q8",
        "text": "Terdakwa mengaku membeli sabu dari seseorang melalui WhatsApp.",
        "ground_truth_case_ids": ["putusan_38_pid", "putusan_106_pid", "putusan_15_pid"]
    },
    {
        "query_id": "Q9",
        "text": "Terdakwa memiliki handphone yang digunakan untuk transaksi narkotika dan dirampas untuk negara atau dimusnahkan.",
        "ground_truth_case_ids": ["putusan_110_pid", "putusan_116_pid", "putusan_118_pid", "putusan_3_pid", "putusan_38_pid", "putusan_61_pid", "putusan_882_pid", "putusan_972_pid", "putusan_754_pid", "putusan_27_pid", "putusan_54_pid", "putusan_62_pid"]
    },
    {
        "query_id": "Q10",
        "text": "Terdakwa terlibat dalam jaringan narkotika dengan jumlah sabu lebih dari 1 kilogram.",
        "ground_truth_case_ids": ["putusan_3_pid", "putusan_754_pid", "putusan_67_pid", "putusan_69_pid"]
    }
]

# Simpan data uji
df_uji = pd.DataFrame(data_uji)

output_dir = "/content/eval"
eval_dir = "/content/results"
os.makedirs(output_dir, exist_ok=True)
os.makedirs(eval_dir, exist_ok=True)

queries_json_path = f"{eval_dir}/queries.json"
with open(queries_json_path, 'w') as f:
    json.dump(data_uji, f, indent=4)
print(f"âœ… File queries.json disimpan ke: {queries_json_path}")

df_uji